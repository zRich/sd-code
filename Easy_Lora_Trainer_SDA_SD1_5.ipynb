{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zRich/sd-code/blob/main/Easy_Lora_Trainer_SDA_SD1_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Easy LoRA trainer SD 1.5 - Stable Diffusion Art\n",
        "### [Guide to use this notebook](https://stable-diffusion-art.com/train-lora/) - Leave comment if you have questions. Speicfy you are using the SD 1.5 LoRA training notebook. Paste the error message.\n",
        "\n",
        "\n",
        "## Log\n",
        "- 03/15/2024: Bug fix. Add LoRA image generation\n",
        "- 02/07/2024: 1-click Easy trainer\n",
        "- 01/30/2024: Bug fix\n"
      ],
      "metadata": {
        "id": "pAVpv903OEoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <font size=\"5\" color=\"orange\"> Upload images and start training </font>\n",
        "#@markdown Begineers: Use a different `Project_folder` each time when you upload the images.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "Project_folder = 'AI_PICS/training/AndyLau' #@param {type:\"string\"}\n",
        "pretained_model_name = 'runwayml/stable-diffusion-v1-5' #@param {type:\"string\"}\n",
        "Image_repeats = 100 #@param {type:\"integer\"}\n",
        "Number_of_epoches = 1 #@param {type:\"integer\"}\n",
        "Learning_rate = 0.0001 #@param {type:\"number\"}\n",
        "Triggering_keyword = 'Andy Lau' #@param {type:\"string\"}\n",
        "Lora_name = 'AndyLau001' #@param {type:\"string\"}\n",
        "Lora_output_path = 'AI_PICS/Lora' #@param {type:\"string\"}\n",
        "Skip_image_upload = False #@param {type:\"boolean\"}\n",
        "#@markdown Image repeats has no effect if you skip image upload.\n",
        "\n",
        "# construct paths\n",
        "projectPath = '/content/drive/MyDrive/' + Project_folder\n",
        "imagePath = projectPath + '/' + str(Image_repeats) + '_'+ Lora_name\n",
        "loraPath = '/content/drive/MyDrive/' + Lora_output_path\n",
        "\n",
        "!mkdir -p {loraPath}\n",
        "\n",
        "\n",
        "def install():\n",
        "  !pip list | grep bitsandbytes > /content/bitsandbytespip.txt\n",
        "  with open('/content/bitsandbytespip.txt', 'r') as file:\n",
        "      if 'bitsandbytes' in file.read():\n",
        "        print('Already installed.')\n",
        "        %cd /content/kohya_ss/\n",
        "        return\n",
        "\n",
        "  print('Installing...')\n",
        "\n",
        "  # Install requirements\n",
        "  !pip install dadaptation==3.1 diffusers[torch]==0.24.0 easygui==0.98.3 einops==0.6.0 fairscale==0.4.13 ftfy==6.1.1 gradio==3.36.1 huggingface-hub==0.19.4\n",
        "  !pip install lion-pytorch==0.0.6 lycoris_lora==1.8.0.dev6 open-clip-torch==2.20.0 prodigyopt==1.0 pytorch-lightning==1.9.0 safetensors==0.3.1 timm==0.6.12\n",
        "  !pip install tk==0.1.0 transformers==4.30.2 voluptuous==0.13.1 wandb==0.15.0 xformers==0.0.20 omegaconf\n",
        "\n",
        "\n",
        "  # Install bitsandbytes\n",
        "  !git clone -b 0.41.0 https://github.com/TimDettmers/bitsandbytes\n",
        "  %cd /content/bitsandbytes\n",
        "  !CUDA_VERSION=118 make cuda11x\n",
        "  !python setup.py install\n",
        "\n",
        "  # Install kohya\n",
        "  %cd /content\n",
        "  !git clone https://github.com/bmaltais/kohya_ss.git\n",
        "  %cd kohya_ss/\n",
        "  !git checkout v21.8.9\n",
        "\n",
        "  # update torchvision to a compatible version\n",
        "  !pip install torch==2.0.1+cu117 torchvision -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "  # add pwd to python path or else blip captioning won't work\n",
        "  %env PYTHONPATH=/env/python:/content/kohya_ss\n",
        "\n",
        "\n",
        "if not Skip_image_upload:\n",
        "  # upload images\n",
        "  import os\n",
        "  from google.colab import files\n",
        "  import shutil\n",
        "  if os.path.exists(projectPath):\n",
        "    raise Exception(f'Error: Project folder {Project_folder} already exists. Please use a different project folder name. \\n')\n",
        "  else:\n",
        "    !mkdir -p {imagePath}\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        dst_path = imagePath + '/' + filename\n",
        "        shutil.move(filename, dst_path)\n",
        "    print('Images uploaded successfully.\\n')\n",
        "\n",
        "\n",
        "\n",
        "install()\n",
        "\n",
        "# auto-captioning\n",
        "if not Skip_image_upload:\n",
        "  !python3 \"finetune/make_captions.py\" --batch_size=\"1\" --num_beams=\"1\"\\\n",
        "                          --top_p=\"0.9\" --max_length=\"75\" --min_length=\"20\" --beam_search\\\n",
        "                          --caption_extension=\".txt\"\\\n",
        "                          {imagePath}\\\n",
        "                          --caption_weights=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\"\n",
        "  # Add preflix to captions\n",
        "  import glob\n",
        "  for file in glob.glob(imagePath + \"/*.txt\"):\n",
        "    !echo {Triggering_keyword}, `cat {file}` > {file}\n",
        "    !cat {file}\n",
        "\n",
        "# Run training\n",
        "!accelerate launch --num_cpu_threads_per_process=2 \"./train_network.py\"     \\\n",
        "                         --enable_bucket --min_bucket_reso=256 --max_bucket_reso=2048               \\\n",
        "                         --pretrained_model_name_or_path={pretained_model_name}           \\\n",
        "                         --train_data_dir={projectPath}         \\\n",
        "                         --resolution=\"512,650\" --output_dir={loraPath}  \\\n",
        "                         --network_alpha=\"64\" --save_model_as=safetensors                           \\\n",
        "                         --network_module=networks.lora --text_encoder_lr=5e-05 --unet_lr={Learning_rate}    \\\n",
        "                         --network_dim=64 --output_name={Lora_name} --lr_scheduler_num_cycles=\"1\"        \\\n",
        "                         --no_half_vae --learning_rate={Learning_rate} --lr_scheduler=\"constant\"           \\\n",
        "                         --train_batch_size=\"3\" --max_train_steps=\"100000\" --save_every_n_epochs=\"99999\"   \\\n",
        "                         --mixed_precision=\"fp16\" --save_precision=\"fp16\" --seed=\"1234\"             \\\n",
        "                         --caption_extension=\".txt\" --cache_latents --optimizer_type=\"AdamW\"        \\\n",
        "                         --max_data_loader_n_workers=\"1\" --clip_skip=2 --bucket_reso_steps=64       \\\n",
        "                         --max_train_epochs={Number_of_epoches}\\\n",
        "                         --mem_eff_attn --xformers --bucket_no_upscale --noise_offset=0.05"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NHsC6v8oxnyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test image generation from LoRA\n",
        "\n",
        "prompt = \"Photo of Andy Lau\" #@param {type:\"string\"}\n",
        "negative_prompt = \"disfigured, deformed\" #@param {type:\"string\"}\n",
        "num_samples = 4 #@param {type:\"number\"}\n",
        "guidance_scale = 7.5 #@param {type:\"number\"}\n",
        "num_inference_steps = 25 #@param {type:\"number\"}\n",
        "height = 512 #@param {type:\"number\"}\n",
        "width = 512 #@param {type:\"number\"}\n",
        "seed = 100 #@param {type:\"number\"}\n",
        "\n",
        "%cd /content/\n",
        "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
        "from matplotlib.pyplot import figure, imshow, axis\n",
        "from matplotlib.image import imread\n",
        "import numpy as np\n",
        "\n",
        "if 'pipe' not in locals():\n",
        "  pipe = StableDiffusionPipeline.from_pretrained(pretained_model_name, safety_checker=None, torch_dtype=torch.float16)\n",
        "  pipe.load_lora_weights(loraPath+'/'+Lora_name + '.safetensors')\n",
        "  pipe.to(\"cuda\")\n",
        "  pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "  g_cuda = None\n",
        "\n",
        "\n",
        "\n",
        "g_cuda = torch.Generator(device='cuda')\n",
        "\n",
        "g_cuda.manual_seed(seed)\n",
        "\n",
        "from torch import autocast\n",
        "with autocast(\"cuda\"), torch.inference_mode():\n",
        "    images = pipe(\n",
        "        prompt,\n",
        "        height=height,\n",
        "        width=width,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_images_per_prompt=num_samples,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        generator=g_cuda\n",
        "    ).images\n",
        "\n",
        "    from ipywidgets import widgets, HBox\n",
        "    from IPython.display import display\n",
        "    for im in images:\n",
        "        display(im)\n",
        "\n",
        "\n",
        "#@title <font size=\"5\" color=\"orange\"> Test LoRA </font>\n",
        "\n",
        "# prompt = 'Photo of Andy Lau' #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "# from diffusers import StableDiffusionPipeline\n",
        "# import torch\n",
        "\n",
        "# pipe = StableDiffusionPipeline.from_pretrained(pretained_model_name, torch_dtype=torch.float16)\n",
        "# pipe.load_lora_weights(loraPath+'/'+Lora_name + '.safetensors')\n",
        "# pipe.to(\"cuda\")\n",
        "\n",
        "# #prompt = f\"Photo of {Triggering_keyword}\"\n",
        "# pipe(prompt,\n",
        "#      num_inference_steps=30,\n",
        "#      guidance_scale=7.5).images[0]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MHrfhTLjJbmK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}